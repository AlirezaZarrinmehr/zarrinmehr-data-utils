{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d83b739-97b7-4daf-8c87-39e84bc23b58",
   "metadata": {},
   "source": [
    "# Zarrinmehr Data Utilities — Python Toolkit for Data Integration and ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3a9f3-9516-4a15-9a00-fba3b10b1cd5",
   "metadata": {},
   "source": [
    "A curated collection of Python utility functions for data engineers and analysts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42476e9d-651c-4e76-903e-fb593c271ade",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c93672-9c3d-44a1-bd60-a4868ee53d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, timedelta, datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pytz\n",
    "import json\n",
    "import pyodbc\n",
    "from requests_oauthlib import OAuth1\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45f1986-cd45-4c4f-ad1c-0ec4871698e3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75c5fe-f214-47ef-ac79-a5da1f1b6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clean_df(\n",
    "clean_df(s3_client = s3_client, s3_bucket_name = s3_bucket_name,\n",
    "\n",
    "read_csv_from_s3(\n",
    "read_csv_from_s3(s3_client = s3_client, \n",
    "\n",
    "upload_to_s3(\n",
    "upload_to_s3(s3_client = s3_client, data = \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfba28c3-5fb6-4efc-b267-94327aa415d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_suiteql_data_via_query(\n",
    "    consumer_key, \n",
    "    consumer_secret, \n",
    "    token_key, \n",
    "    token_secret, \n",
    "    realm, \n",
    "    query, \n",
    "    limit=1000\n",
    "):\n",
    "    auth = OAuth1(\n",
    "        consumer_key,\n",
    "        consumer_secret,\n",
    "        token_key,\n",
    "        token_secret,\n",
    "        realm=realm,\n",
    "        signature_method='HMAC-SHA256'\n",
    "    )\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json',\n",
    "        'Prefer': 'transient'\n",
    "    }\n",
    "    offset = 0\n",
    "    hasMore = True\n",
    "    all_items = []\n",
    "    total_results =0\n",
    "    with tqdm(total=total_results, desc=\"Fetching data from NetSuite\", unit=\"records\") as pbar:\n",
    "        while hasMore:\n",
    "            suiteql_url = f'https://{realm}.suitetalk.api.netsuite.com/services/rest/query/v1/suiteql?limit={limit}&offset={offset}'\n",
    "            response = requests.post(suiteql_url, auth=auth, headers=headers, json={\"q\": query})\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                hasMore = result.get('hasMore', False)\n",
    "                count = result.get('count', 0)\n",
    "                items = result.get('items', [])\n",
    "                all_items.extend(items)\n",
    "                offset += limit\n",
    "                total_results = result.get('totalResults', 0)\n",
    "                pbar.total = total_results\n",
    "                pbar.update(count)\n",
    "                pbar.refresh()\n",
    "            else:\n",
    "                raise Exception(f'Error executing SuiteQL query: {response.status_code}, {response.text}')\n",
    "                # hasMore = False\n",
    "    # Convert results to DataFrame and return\n",
    "    df = pd.DataFrame(all_items)\n",
    "    return df\n",
    "\n",
    "def load_data_via_query(\n",
    "    sql_query, \n",
    "    connection_string, \n",
    "    chunksize=1000\n",
    "):\n",
    "    print(f\"Running {sql_query}\")\n",
    "    chunks = []\n",
    "    with pyodbc.connect(connection_string) as conn:\n",
    "        total_rows = pd.read_sql_query(\"SELECT COUNT(*) FROM ({}) subquery\".format(sql_query), conn).iloc[0, 0]\n",
    "        total_chunks = (total_rows // chunksize) + (total_rows % chunksize > 0)\n",
    "        for chunk in tqdm(pd.read_sql_query(sql_query, conn, chunksize=chunksize), total=total_chunks):\n",
    "            chunks.append(chunk)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    df.columns = df.columns.str.title()\n",
    "    return df\n",
    "\n",
    "def read_csv_from_s3(\n",
    "    bucket_name, \n",
    "    object_key, \n",
    "    s3_client, \n",
    "    encoding='utf-8', \n",
    "    is_csv_file=True, \n",
    "    low_memory = True, \n",
    "    dtype_str=False\n",
    "):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    file_size = obj['ContentLength']\n",
    "    progress = tqdm(total=file_size, unit='B', unit_scale=True, desc=f'Downloading {object_key}')\n",
    "    def stream_with_progress(bytes_io):\n",
    "        while True:\n",
    "            chunk = bytes_io.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            progress.update(len(chunk))\n",
    "            yield chunk\n",
    "        progress.close()\n",
    "    body = obj['Body']\n",
    "    if is_csv_file:\n",
    "        stream = stream_with_progress(body)\n",
    "        csv_string = b''.join(stream).decode(encoding)\n",
    "        csv_buffer = io.StringIO(csv_string)\n",
    "        if dtype_str:\n",
    "            df = pd.read_csv(csv_buffer, sep=',', quotechar='\"', quoting=csv.QUOTE_ALL, low_memory=low_memory, dtype=str, na_values=[''], keep_default_na=False)\n",
    "        else:    \n",
    "            df = pd.read_csv(csv_buffer, sep=',', quotechar='\"', quoting=csv.QUOTE_ALL, low_memory=low_memory)        \n",
    "    else:\n",
    "        stream = stream_with_progress(body)\n",
    "        xlsx_data = b''.join(stream)\n",
    "        xlsx_buffer = io.BytesIO(xlsx_data)\n",
    "        df = pd.read_excel(xlsx_buffer, engine='openpyxl')\n",
    "    return df\n",
    "\n",
    "def upload_to_s3(\n",
    "    data, \n",
    "    bucket_name, \n",
    "    object_key, \n",
    "    s3_client, \n",
    "    CreateS3Bucket=False\n",
    "):\n",
    "    \n",
    "    if CreateS3Bucket:\n",
    "        try:\n",
    "            buckets = pd.DataFrame(s3_client.list_buckets()[\"Buckets\"])\n",
    "            if bucket_name not in buckets.Name.to_list():\n",
    "                s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': AWS_REGION})\n",
    "                prompt = f'{print_date_time()}\\t\\tBucket \"{bucket_name}\" created successfully'\n",
    "                print(prompt)\n",
    "                write_file('log.txt' , f\"{prompt}\")\n",
    "            else:\n",
    "                prompt = f'{print_date_time()}\\t\\tBucket \"{bucket_name}\" already exists'\n",
    "                print(prompt)\n",
    "                write_file('log.txt' , f\"{prompt}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            prompt = f'{print_date_time()}\\t\\tFailed to create bucket \"{bucket_name}\". Error: {str(e)}.'\n",
    "            print(prompt)\n",
    "            write_file('log.txt' , f\"{prompt}\")\n",
    "\n",
    "    clean_data = data.copy()\n",
    "    # for col in clean_data.columns:\n",
    "    for col in clean_data.select_dtypes(include=['object', 'string']).columns:\n",
    "        clean_data[col] = clean_data[col].fillna('').astype(str).str.replace(r'\\r\\n|\\r|\\n', ' ', regex=True)\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    clean_data.to_csv(csv_buffer, index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_ALL, escapechar='\\\\', encoding='utf-8')\n",
    "    csv_buffer.seek(0)\n",
    "    data_size = len(csv_buffer.getvalue())\n",
    "    \n",
    "    with tqdm(total=data_size, unit='B', unit_scale=True, desc=f'Uploading \"{object_key}\" to S3') as progress:\n",
    "        \n",
    "        def callback(bytes_transferred):\n",
    "            progress.update(bytes_transferred)\n",
    "            \n",
    "        bytes_buffer = io.BytesIO(csv_buffer.getvalue().encode())\n",
    "        s3_client.upload_fileobj(\n",
    "            Fileobj=bytes_buffer,\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "            Callback=callback\n",
    "        )\n",
    "\n",
    "def clean_df(\n",
    "    s3_client,\n",
    "    s3_bucket_name,\n",
    "    df,\n",
    "    df_name,\n",
    "    id_column=None,\n",
    "    additional_date_columns=None,\n",
    "    zip_code_columns=None,\n",
    "    state_columns=None,\n",
    "    keep_invalid_as_null=True,\n",
    "    numeric_id=False, \n",
    "    just_useful_columns=False\n",
    "):\n",
    "    col_to_date = [col for col in df.columns if 'date' in col.lower()] + additional_date_columns\n",
    "    col_to_date = list(set(col_to_date))\n",
    "    for col in col_to_date:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    for col in set(df.columns)-set(id_column):\n",
    "        df[col] = df[col].apply(lambda x: x.strip().upper() if isinstance(x, str) else x)\n",
    "    if id_column:\n",
    "        if numeric_id:\n",
    "            invalid_mask = ~df[id_column].astype(str).apply(lambda x: x.str.isdigit()).any(axis=1)\n",
    "            # invalid_mask = ~df[id_column].astype(str).str.isdigit()\n",
    "            invalid_id = df[invalid_mask]\n",
    "            df = df[~invalid_mask].copy()\n",
    "            print(f\"invalid {id_column} found and removed: {len(invalid_id)}\")\n",
    "            if len(invalid_id)>0:\n",
    "                # upload_to_s3(s3_client = s3_client,  data = invalid_id, bucket_name = s3_bucket_name + '-c', object_key = f\"{df_name}_invalid_{id_column}.csv\", CreateS3Bucket=True)\n",
    "                upload_to_s3(s3_client = s3_client,  data = invalid_id, bucket_name = s3_bucket_name + '-c', object_key = f\"{df_name}_invalid_{re.sub(r'[^a-zA-Z0-9]', '_', '_'.join(id_column))}.csv\", CreateS3Bucket=True)\n",
    "        duplicated_mask = df[id_column].duplicated()\n",
    "        duplicated_id = df[duplicated_mask]\n",
    "        df = df[~duplicated_mask].copy()\n",
    "        print(f\"duplicated {id_column} found and removed: {len(duplicated_id)}\")\n",
    "        if len(duplicated_id)>0:\n",
    "            # upload_to_s3(s3_client = s3_client,  data = duplicated_id, bucket_name = s3_bucket_name + '-c', object_key = f\"{df_name}_duplicated_{id_column}.csv\", CreateS3Bucket=True)\n",
    "            upload_to_s3(s3_client = s3_client,  data = duplicated_id, bucket_name = s3_bucket_name + '-c', object_key = f\"{df_name}_duplicated_{re.sub(r'[^a-zA-Z0-9]', '_', '_'.join(id_column))}.csv\", CreateS3Bucket=True)\n",
    "    if zip_code_columns:\n",
    "        invalid_zip_codes = pd.DataFrame()\n",
    "        valid_us_zip_regex = r\"^\\d{5}(\\d{4})?$\"\n",
    "        valid_ca_zip_regex = r\"^[A-Za-z]\\d[A-Za-z](\\d[A-Za-z]\\d)?$\"        \n",
    "        for col in zip_code_columns:\n",
    "            df[col] = df[col].astype(str).str.replace(' ','')\n",
    "            df[col] = df[col].astype(str).str.replace('-','')            \n",
    "            invalid_mask = ~(df[col].str.match(valid_us_zip_regex) | df[col].str.match(valid_ca_zip_regex))\n",
    "            invalid_zip_codes = pd.concat([invalid_zip_codes, df[invalid_mask]], ignore_index=True)\n",
    "            df = df[~invalid_mask].copy()\n",
    "            df[col] = df[col].apply(lambda x: \\\n",
    "                                    x[0:5]+'-'+x[0:4] if isinstance(x, str) and re.match(valid_us_zip_regex, x) else \\\n",
    "                                    x[0:3]+' '+x[3:6] if isinstance(x, str) and re.match(valid_ca_zip_regex, x) else  \\\n",
    "                                    x)\n",
    "        print(f\"invalid_zip_codes found: {len(invalid_zip_codes)}\")\n",
    "        upload_to_s3(s3_client = s3_client,  data = invalid_zip_codes, bucket_name = s3_bucket_name + '-c', object_key = f\"{df_name}_invalid_zip_codes.csv\", CreateS3Bucket=True)\n",
    "        if keep_invalid_as_null:\n",
    "            for col in zip_code_columns:\n",
    "                invalid_zip_codes[col] = np.nan\n",
    "            df = pd.concat([df, invalid_zip_codes], ignore_index=True)        \n",
    "    if state_columns:\n",
    "        invalid_states = pd.DataFrame()\n",
    "        valid_us_states = { \"DC\": \"District of Columbia\", \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\", \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\", \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\", \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\" }\n",
    "        valid_ca_states = { \"AB\": \"Alberta\", \"BC\": \"British Columbia\", \"MB\": \"Manitoba\", \"NB\": \"New Brunswick\", \"NL\": \"Newfoundland and Labrador\", \"NS\": \"Nova Scotia\", \"ON\": \"Ontario\", \"PE\": \"Prince Edward Island\", \"QC\": \"Quebec\", \"SK\": \"Saskatchewan\", \"NT\": \"Northwest Territories\", \"NU\": \"Nunavut\", \"YT\": \"Yukon\" }\n",
    "        for col in state_columns:\n",
    "            df[col] = df[col].astype(str).str.replace(' ','')\n",
    "            df[col] = df[col].astype(str).str.replace('-','')\n",
    "            invalid_mask = ~df[col].isin(set(valid_us_states.keys()).union(valid_ca_states.keys()))\n",
    "            invalid_states = pd.concat([invalid_states, df[invalid_mask]], ignore_index=True)\n",
    "            df = df[~invalid_mask].copy()\n",
    "        print(f\"invalid_states found: {len(invalid_states)}\")\n",
    "        upload_to_s3(s3_client = s3_client,  data = invalid_states, bucket_name = s3_bucket_name + '-c', object_key = f\"{df_name}_invalid_states.csv\", CreateS3Bucket=True)\n",
    "        if keep_invalid_as_null:\n",
    "            for col in state_columns:\n",
    "                invalid_states[col] = np.nan\n",
    "            df = pd.concat([df, invalid_states], ignore_index=True)\n",
    "    if just_useful_columns:\n",
    "        useful_columns = find_useful_columns(df)\n",
    "        print(f\"{len(useful_columns)} useful variables found!\")\n",
    "        df = df[useful_columns]\n",
    "    return df\n",
    "\n",
    "def find_useful_columns(\n",
    "    df\n",
    "):\n",
    "    useful_cols = [col for col in df.columns if not (df[col].isna().sum() == df.shape[0] or df[col].value_counts().iloc[0] == df.shape[0])]\n",
    "    return useful_cols\n",
    "\n",
    "def group(\n",
    "    x, \n",
    "    quantile_values\n",
    "):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    elif x <= quantile_values[1]:\n",
    "        return f\"{quantile_values[0]:03}-{quantile_values[1]:03}\"\n",
    "    elif x <= quantile_values[2]:\n",
    "        return f\"{quantile_values[1]+1:03}-{quantile_values[2]:03}\"\n",
    "    elif x <= quantile_values[3]:\n",
    "        return f\"{quantile_values[2]+1:03}-{quantile_values[3]:03}\"\n",
    "    elif x <= quantile_values[4]:\n",
    "        return f\"{quantile_values[3]+1:03}-{quantile_values[4]:03}\"\n",
    "    else:\n",
    "        return f\"{quantile_values[4]+1:03}+\"\n",
    "\n",
    "def find_unique_value_columns(\n",
    "    dataframe\n",
    "):\n",
    "    unique_value_columns = []\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].nunique() == len(dataframe):\n",
    "            unique_value_columns.append(column)\n",
    "    return unique_value_columns\n",
    "\n",
    "def write_file(\n",
    "    filename, \n",
    "    data\n",
    "):\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'a', encoding='utf-8') as f:\n",
    "            f.write('\\n' + data)\n",
    "    else:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(data)\n",
    "\n",
    "def print_date_time():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%D-%H:%M:%S\")\n",
    "    data = \"Current Time = \" + current_time\n",
    "    return data\n",
    "\n",
    "def correctCompleteDates(\n",
    "    df , \n",
    "    orderStatusCol, \n",
    "    orderDateCol, \n",
    "    completeDateCol, \n",
    "    shipDateCol, \n",
    "    invoiceDateCol, \n",
    "    lastModDateCol, \n",
    "    postCompletionStatuses\n",
    "):\n",
    "    df[orderDateCol] = pd.to_datetime(df[orderDateCol], errors='coerce')\n",
    "    df[completeDateCol] = pd.to_datetime(df[completeDateCol], errors='coerce')\n",
    "    df[shipDateCol] = pd.to_datetime(df[shipDateCol], errors='coerce')\n",
    "    df[invoiceDateCol] = pd.to_datetime(df[invoiceDateCol], errors='coerce')\n",
    "    df[lastModDateCol] = pd.to_datetime(df[lastModDateCol], errors='coerce')                           \n",
    "    def correctCompleteDate(row):\n",
    "        orderStatus = row[orderStatusCol]\n",
    "        orderDate = row[orderDateCol]\n",
    "        completeDate = row[completeDateCol]\n",
    "        arriveDate = row[shipDateCol]\n",
    "        invoiceDate = row[invoiceDateCol]\n",
    "        lastModDate = row[lastModDateCol]\n",
    "        if completeDate >= orderDate:\n",
    "            return completeDate\n",
    "        elif completeDate < orderDate and arriveDate >= orderDate:\n",
    "            return arriveDate\n",
    "        elif completeDate < orderDate and invoiceDate >= orderDate:\n",
    "            return invoiceDate\n",
    "        elif orderStatus in postCompletionStatuses and lastModDate >= orderDate:\n",
    "            return lastModDate\n",
    "        else:\n",
    "            return None\n",
    "    df['CorrectedCompletedDate'] = df.apply(correctCompleteDate, axis=1)\n",
    "    return df\n",
    "\n",
    "def convert_to_int_or_keep(\n",
    "    x\n",
    "):\n",
    "    try:\n",
    "        return int(pd.to_numeric(x))\n",
    "    except (ValueError, TypeError):\n",
    "        return x\n",
    "\n",
    "state_map = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n",
    "    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n",
    "}\n",
    "\n",
    "abbrev_map = {v: v for v in state_map.values()}\n",
    "state_map.update(abbrev_map)\n",
    "\n",
    "def extract_state(\n",
    "    text\n",
    "):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    for key, value in state_map.items():\n",
    "        match = re.search(rf'\\b{key.lower()}\\b', text.lower())\n",
    "        if match:\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def read_iif_from_s3(\n",
    "    bucket_name, \n",
    "    object_key, \n",
    "    s3_client, \n",
    "    encoding='Windows-1252'\n",
    "):\n",
    "\n",
    "    iif_obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    file_size = iif_obj['ContentLength']   \n",
    "    progress = tqdm(total=file_size, unit='B', unit_scale=True, desc=f'Downloading {object_key}')\n",
    "    \n",
    "    def stream_with_progress(bytes_io):\n",
    "        while True:\n",
    "            chunk = bytes_io.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            progress.update(len(chunk))\n",
    "            yield chunk\n",
    "        progress.close()\n",
    "    \n",
    "    body = iif_obj['Body']\n",
    "    stream = stream_with_progress(body)\n",
    "    iif_string = b''.join(stream).decode(encoding)  \n",
    "    iif_buffer = io.StringIO(iif_string)\n",
    "    columns = [f'Column{i}' for i in range(1, 101)]\n",
    "    df = pd.read_csv(iif_buffer, delimiter='\\t', names=columns, encoding=encoding)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_address(\n",
    "    df\n",
    "):\n",
    "    def extract_address_name_city_state_zip(address):\n",
    "        address = str(address)\n",
    "        if address is None:\n",
    "            return None, None, None, None \n",
    "        try:\n",
    "            address = address.upper()\n",
    "            us_zip_pattern = r'\\b\\d{5}\\b'\n",
    "            ca_zip_pattern = r'[A-Za-z]\\d[A-Za-z]\\s?\\d[A-Za-z]\\d'\n",
    "            matches = re.findall(us_zip_pattern, address) or re.findall(ca_zip_pattern, address)\n",
    "            zip_code = matches[-1]\n",
    "            address = ''.join(re.split(zip_code, address)[:-1])\n",
    "            valid_us_states = {'DISTRICT OF COLUMBIA': 'DC', 'ALABAMA': 'AL', 'ALASKA': 'AK', 'ARIZONA': 'AZ', 'ARKANSAS': 'AR', 'CALIFORNIA': 'CA', 'COLORADO': 'CO', 'CONNECTICUT': 'CT', 'DELAWARE': 'DE', 'FLORIDA': 'FL', 'GEORGIA': 'GA', 'HAWAII': 'HI', 'IDAHO': 'ID', 'ILLINOIS': 'IL', 'INDIANA': 'IN', 'IOWA': 'IA', 'KANSAS': 'KS', 'KENTUCKY': 'KY', 'LOUISIANA': 'LA', 'MAINE': 'ME', 'MARYLAND': 'MD', 'MASSACHUSETTS': 'MA', 'MICHIGAN': 'MI', 'MINNESOTA': 'MN', 'MISSISSIPPI': 'MS', 'MISSOURI': 'MO', 'MONTANA': 'MT', 'NEBRASKA': 'NE', 'NEVADA': 'NV', 'NEW HAMPSHIRE': 'NH', 'NEW JERSEY': 'NJ', 'NEW MEXICO': 'NM', 'NEW YORK': 'NY', 'NORTH CAROLINA': 'NC', 'NORTH DAKOTA': 'ND', 'OHIO': 'OH', 'OKLAHOMA': 'OK', 'OREGON': 'OR', 'PENNSYLVANIA': 'PA', 'RHODE ISLAND': 'RI', 'SOUTH CAROLINA': 'SC', 'SOUTH DAKOTA': 'SD', 'TENNESSEE': 'TN', 'TEXAS': 'TX', 'UTAH': 'UT', 'VERMONT': 'VT', 'VIRGINIA': 'VA', 'WASHINGTON': 'WA', 'WEST VIRGINIA': 'WV', 'WISCONSIN': 'WI', 'WYOMING': 'WY'}\n",
    "            valid_ca_states = {'ALBERTA': 'AB', 'BRITISH COLUMBIA': 'BC', 'MANITOBA': 'MB', 'NEW BRUNSWICK': 'NB', 'NEWFOUNDLAND AND LABRADOR': 'NL', 'NOVA SCOTIA': 'NS', 'ONTARIO': 'ON', 'PRINCE EDWARD ISLAND': 'PE', 'QUEBEC': 'QC', 'SASKATCHEWAN': 'SK', 'NORTHWEST TERRITORIES': 'NT', 'NUNAVUT': 'NU', 'YUKON': 'YT'}\n",
    "            valid_states = {**valid_us_states, **valid_ca_states}\n",
    "            state_pattern = re.compile(r'\\b(' + '|'.join(re.escape(state) for state in valid_states.keys()) + r')\\b', re.IGNORECASE)\n",
    "            address = state_pattern.sub(lambda match: valid_states[match.group(0).upper()], address)\n",
    "            state_pattern = r'\\b[a-zA-Z]{2}\\b'\n",
    "            matches = re.findall(state_pattern, address)\n",
    "            state = matches[-1]\n",
    "            address = ''.join(re.split(state, address)[:-1])\n",
    "            city = [i for i in address.strip().split(',') if i != ''][-1].strip()\n",
    "            address = ''.join(re.split(city, address)[:-1])\n",
    "            ship_name = [i for i in address.strip().split(',') if i != ''][0].strip()\n",
    "            return ship_name, city, state, zip_code\n",
    "        except:\n",
    "            return None, None, None, None \n",
    "        \n",
    "    addresses = {'BillAddressBlockAddr':'billingAddress', 'ShipAddressBlockAddr':'ShippingAddress', 'BADDR':'billingAddress', 'SADDR':'ShippingAddress', 'ADDR':'Address'}\n",
    "    for key, value in addresses.items():\n",
    "        AddressCols = [i for i in df.columns if key in i]\n",
    "        if AddressCols:\n",
    "            df[value] = df[AddressCols].agg(lambda x: ', '.join(x.dropna()), axis=1)\n",
    "            df.drop(columns = AddressCols, inplace=True)\n",
    "            if not df.empty:\n",
    "                df[[f'{value}Name', f'{value}City', f'{value}State', f'{value}Zip']] = df[value].apply(extract_address_name_city_state_zip).to_list()\n",
    "            else:\n",
    "                df[[f'{value}Name', f'{value}City', f'{value}State', f'{value}Zip']] = None\n",
    "    return df\n",
    "\n",
    "def extract_lists(\n",
    "    transactions, \n",
    "    table\n",
    "):\n",
    "    df = transactions.copy()\n",
    "    columns = [ df[df['Column1'] == f'!{table}'][col].iloc[0] if not pd.isna(df[df['Column1'] == f'!{table}'][col].iloc[0]) else col for col in df.columns ]\n",
    "    df.columns = columns\n",
    "    df = df[df[f'!{table}'] == f'{table}']\n",
    "    df = df[[i for i in df.columns if 'Column' not in i]].copy()\n",
    "    df = clean_address(df)\n",
    "    return df\n",
    "\n",
    "def extract_transaction_header_line(\n",
    "    transactions, \n",
    "    trns_type\n",
    "):\n",
    "    df = transactions.copy()\n",
    "    df_columns = [ df[df['Column1'] == f'!TRNS'][col].item() if not pd.isna(df[df['Column1'] == f'!TRNS'][col].item()) else col for col in df.columns ]\n",
    "    df_line_columns = [ df[df['Column1'] == f'!SPL'][col].item() if not pd.isna(df[df['Column1'] == f'!SPL'][col].item()) else col for col in df.columns ]\n",
    "    df = df[df['Column3'] == f'{trns_type}']\n",
    "    df = df[~df['Column2'].duplicated()].copy()\n",
    "    for Col in ['Column2', 'Column9']:\n",
    "        df.loc[:, Col] = df[Col].fillna('').apply(convert_to_int_or_keep).astype('str')\n",
    "        df.loc[df['Column1']=='SPL', Col] = None\n",
    "        df.loc[:, Col] = df[Col].ffill()\n",
    "    df_line = df[df['Column1'] == 'SPL'].copy()\n",
    "    df_line.columns = df_line_columns\n",
    "    df_line = df_line[[i for i in df_line.columns if 'Column' not in i]].copy()\n",
    "    if not df_line.empty:\n",
    "        df_line = clean_address(df_line)\n",
    "    df = df[df['Column1'] == 'TRNS'].copy()\n",
    "    df.columns = df_columns\n",
    "    df = df[[i for i in df.columns if 'Column' not in i]].copy()\n",
    "    # if not df.empty:\n",
    "    df = clean_address(df)\n",
    "    return df, df_line\n",
    "\n",
    "def replace_date(\n",
    "    row, \n",
    "    date_col, \n",
    "    year_col=None, \n",
    "    month_col=None,\n",
    "    day_col=None\n",
    "):\n",
    "    year = row[date_col].year\n",
    "    month = row[date_col].month\n",
    "    day = row[date_col].day\n",
    "    if year_col and year_col in row:\n",
    "        year = row[year_col]\n",
    "    if month_col and month_col in row:\n",
    "        month = row[month_col]\n",
    "    if day_col and day_col in row:\n",
    "        day = row[day_col]\n",
    "    try:\n",
    "        return row[date_col].replace(year=year, month=month, day=day)\n",
    "    except ValueError:\n",
    "        last_valid_day = (pd.Timestamp(f\"{year}-{month}-01\") + pd.offsets.MonthEnd(0)).day\n",
    "        return row[date_col].replace(year=year, month=month, day=min(day, last_valid_day))\n",
    "\n",
    "def wait_for_cluster_available(\n",
    "    redshift_client,\n",
    "    redshift_cluster_identifier\n",
    "):\n",
    "    waiter = redshift_client.get_waiter('cluster_available')\n",
    "    try:\n",
    "        prompt = f'{print_date_time()}\\t\\tWaiting for the Redshift cluster \"{redshift_cluster_identifier}\" to become available...'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "        waiter.wait(ClusterIdentifier=redshift_cluster_identifier)\n",
    "        response = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "        cluster_status = response['Clusters'][0]['ClusterStatus']\n",
    "        if cluster_status == 'available':\n",
    "            prompt = f'{print_date_time()}\\t\\tCluster \"{redshift_cluster_identifier}\" is now available.'\n",
    "            print(prompt)\n",
    "            write_file('log.txt', f\"{prompt}\")\n",
    "        else:\n",
    "            prompt = f'{print_date_time()}\\t\\tCluster \"{redshift_cluster_identifier}\" is not available. Current status: {cluster_status}'\n",
    "            print(prompt)\n",
    "            write_file('log.txt', f\"{prompt}\")\n",
    "            raise ValueError(f'Cluster \"{redshift_cluster_identifier}\" is not available. Current status: \"{cluster_status}\"')\n",
    "    except Exception as e:\n",
    "        prompt = f'{print_date_time()}\\t\\tError waiting for cluster to become available: {e}'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "\n",
    "def create_iam_role(\n",
    "    iam_client,\n",
    "    role_name,\n",
    "    trust_policy\n",
    "):\n",
    "    try:\n",
    "        response = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "        )\n",
    "        prompt = f'{print_date_time()}\\t\\tRole \"{role_name}\" created successfully.'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "        return response['Role']['Arn']\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        prompt = f'{print_date_time()}\\t\\t⚠️ Role \"{role_name}\" already exists.'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "        return iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "\n",
    "def attach_policies_to_role(\n",
    "    iam_client,\n",
    "    role_name, \n",
    "    role_policies\n",
    "):\n",
    "    for policy in role_policies:\n",
    "        try:\n",
    "            iam_client.attach_role_policy(\n",
    "                RoleName=role_name,\n",
    "                PolicyArn=f'arn:aws:iam::aws:policy/{policy}'\n",
    "            )\n",
    "            prompt = f'{print_date_time()}\\t\\tPolicy \"{policy}\" attached to role \"{role_name}\".'\n",
    "            print(prompt)\n",
    "            write_file('log.txt', f\"{prompt}\")\n",
    "        except iam_client.exceptions.NoSuchEntityException as e:\n",
    "            prompt = f'{print_date_time()}\\t\\tError attaching policy \"{policy}\": {str(e)}'\n",
    "            print(prompt)\n",
    "            write_file('log.txt', f\"{prompt}\")\n",
    "\n",
    "def associate_role_with_redshift(\n",
    "    redshift_client,\n",
    "    redshift_iam_role_arn, \n",
    "    redshift_cluster_identifier, \n",
    "    timeout=20, \n",
    "    check_interval=2\n",
    "):\n",
    "    try:\n",
    "        response = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "        current_roles = response[\"Clusters\"][0].get(\"IamRoles\", [])\n",
    "        role_associated = any(role['IamRoleArn'] == redshift_iam_role_arn for role in current_roles) \n",
    "        if role_associated:\n",
    "            prompt = f'{print_date_time()}\\t\\t⚠️ Role \"{redshift_iam_role_arn}\" is already associated with the Redshift cluster \"{redshift_cluster_identifier}\".'\n",
    "            print(prompt)\n",
    "            write_file('log.txt', f\"{prompt}\")\n",
    "        else:\n",
    "            redshift_client.modify_cluster_iam_roles(\n",
    "                ClusterIdentifier=redshift_cluster_identifier,\n",
    "                AddIamRoles=[redshift_iam_role_arn]\n",
    "            )\n",
    "            prompt = f'{print_date_time()}\\t\\tAttempting to associate the role \"{redshift_iam_role_arn}\" with the Redshift cluster \"{redshift_cluster_identifier}\".'\n",
    "            print(prompt)\n",
    "            write_file('log.txt', f\"{prompt}\")\n",
    "            wait_for_cluster_available(redshift_client, redshift_cluster_identifier) \n",
    "            elapsed_time = 0\n",
    "            role_associated = False\n",
    "            while elapsed_time < timeout and not role_associated:\n",
    "                time.sleep(check_interval)\n",
    "                elapsed_time += check_interval\n",
    "                response = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "                updated_roles = response[\"Clusters\"][0].get(\"IamRoles\", [])\n",
    "                role_associated = any(role['IamRoleArn'] == redshift_iam_role_arn for role in updated_roles)\n",
    "                if role_associated:\n",
    "                    prompt = f'{print_date_time()}\\t\\t✅ Role \"{redshift_iam_role_arn}\" has been successfully associated with the Redshift cluster \"{redshift_cluster_identifier}\".'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt', f\"{prompt}\")\n",
    "                else:\n",
    "                    prompt = f'{print_date_time()}\\t\\t⏳ Waiting for IAM role \"{redshift_iam_role_arn}\" to be associated with the Redshift cluster \"{redshift_cluster_identifier}\". Retrying...'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt', f\"{prompt}\")\n",
    "            if not role_associated:\n",
    "                prompt = f'{print_date_time()}\\t\\t❌ Timeout reached. Role \"{redshift_iam_role_arn}\" was not associated with the Redshift cluster \"{redshift_cluster_identifier}\" within {timeout} seconds.'\n",
    "                print(prompt)\n",
    "                write_file('log.txt', f\"{prompt}\")\n",
    "    except redshift_client.exceptions.ClusterNotFoundFault:\n",
    "        prompt = f'{print_date_time()}\\t\\tError: Redshift cluster \"{redshift_cluster_identifier}\" not found.'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "    except Exception as e:\n",
    "        prompt = f'{print_date_time()}\\t\\tError associating role: {str(e)}'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "        \n",
    "def add_inbound_rule(\n",
    "    redshift_client, \n",
    "    ec2_client, \n",
    "    redshift_cluster_identifier\n",
    "):\n",
    "    response = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    security_group_id = response[\"Clusters\"][0][\"VpcSecurityGroups\"][0][\"VpcSecurityGroupId\"]\n",
    "    security_groups = ec2_client.describe_security_groups(GroupIds=[security_group_id])\n",
    "    existing_rules = security_groups[\"SecurityGroups\"][0][\"IpPermissions\"]\n",
    "    rule_exists = any(\n",
    "        rule[\"FromPort\"] == 5439 and rule[\"ToPort\"] == 5439 and rule[\"IpProtocol\"] == \"tcp\"\n",
    "        for rule in existing_rules\n",
    "    )\n",
    "    if not rule_exists:\n",
    "        prompt = f'{print_date_time()}\\t\\tAdding inbound rule for security group to allow access from 0.0.0.0/0...'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")   \n",
    "        ec2_client.authorize_security_group_ingress(\n",
    "            GroupId=security_group_id,\n",
    "            IpProtocol=\"tcp\",\n",
    "            FromPort=5439,\n",
    "            ToPort=5439,\n",
    "            CidrIp=\"0.0.0.0/0\"\n",
    "        )\n",
    "        prompt = f'{print_date_time()}\\t\\t✅ Inbound rule for 0.0.0.0/0 added to security group \"{security_group_id}\".'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "    else:\n",
    "        prompt = f'{print_date_time()}\\t\\t⚠️ Security group rule already exists, skipping.'\n",
    "        print(prompt)\n",
    "        write_file('log.txt', f\"{prompt}\")\n",
    "\n",
    "def turn_on_case_sensitivity(\n",
    "    redshift_client,\n",
    "    redshift_cluster_identifier\n",
    "):\n",
    "    parameter_group_name = f'{redshift_cluster_identifier}-params'\n",
    "    parameter_group_family = 'redshift-2.0'\n",
    "    existing_groups = redshift_client.describe_cluster_parameter_groups()['ParameterGroups']\n",
    "    group_names = [group['ParameterGroupName'] for group in existing_groups]\n",
    "    if parameter_group_name not in group_names:  \n",
    "        response = redshift_client.create_cluster_parameter_group(\n",
    "            ParameterGroupName=parameter_group_name,\n",
    "            ParameterGroupFamily=parameter_group_family,\n",
    "            Description=f'Param group for {redshift_cluster_identifier}'\n",
    "        )\n",
    "        print(f\"Created parameter group '{parameter_group_name}'\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Parameter group '{parameter_group_name}' already exists. Skipping creation.\")\n",
    "    wait_for_cluster_available(redshift_client, redshift_cluster_identifier)\n",
    "    response = redshift_client.modify_cluster_parameter_group(\n",
    "        ParameterGroupName=parameter_group_name,\n",
    "        Parameters=[\n",
    "            {\n",
    "                'ParameterName': 'enable_case_sensitive_identifier',\n",
    "                'ParameterValue': 'true',\n",
    "                'ApplyType': 'static'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Modified parameter group '{parameter_group_name}'\")\n",
    "    wait_for_cluster_available(redshift_client, redshift_cluster_identifier)\n",
    "    cluster = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)['Clusters'][0]\n",
    "    current_parameter_group = cluster['ClusterParameterGroups'][0]['ParameterGroupName']\n",
    "    if current_parameter_group != parameter_group_name:\n",
    "        response = redshift_client.modify_cluster(\n",
    "            ClusterIdentifier=redshift_cluster_identifier,\n",
    "            ClusterParameterGroupName=parameter_group_name\n",
    "        )\n",
    "        print(f\"Modified cluster '{redshift_cluster_identifier}' with parameter group '{parameter_group_name}'\")\n",
    "        response = redshift_client.reboot_cluster(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    else:\n",
    "        print(f\"Cluster '{redshift_cluster_identifier}' already has the parameter group '{parameter_group_name}' associated.\")\n",
    "    wait_for_cluster_available(redshift_client, redshift_cluster_identifier)\n",
    "\n",
    "def upload_to_redshift(\n",
    "    s3_client,\n",
    "    redshift_client,\n",
    "    iam_client,\n",
    "    ec2_client,\n",
    "    trust_policy,\n",
    "    role_policies,\n",
    "    s3_bucket_names,\n",
    "    redshift_cluster_identifier,\n",
    "    redshift_db_name,\n",
    "    redshift_master_username,\n",
    "    redshift_master_password,\n",
    "    role_name,\n",
    "    redshift_node_type,\n",
    "    redshift_cluster_type,\n",
    "    redshift_number_of_nodes,\n",
    "    createRedshiftCluster=False,\n",
    "    max_allowed_length= 870\n",
    "):\n",
    "    try:\n",
    "        redshift_iam_role_arn = create_iam_role(iam_client, role_name, trust_policy)\n",
    "        attach_policies_to_role(iam_client, role_name, role_policies)\n",
    "        response = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "        cluster_status = response[\"Clusters\"][0][\"ClusterStatus\"]\n",
    "        prompt = f'{print_date_time()}\\t\\tCluster \"{redshift_cluster_identifier}\" exists. Status: {cluster_status}'\n",
    "        print(prompt)\n",
    "        write_file('log.txt' , f\"{prompt}\")\n",
    "    except redshift_client.exceptions.ClusterNotFoundFault:\n",
    "        if createRedshiftCluster:\n",
    "            prompt = f'{print_date_time()}\\t\\tCluster \"{redshift_cluster_identifier}\" not found. Creating a new one...'\n",
    "            print(prompt)\n",
    "            write_file('log.txt' , f\"{prompt}\")\n",
    "            redshift_client.create_cluster(\n",
    "                ClusterIdentifier=redshift_cluster_identifier,\n",
    "                NodeType=redshift_node_type,\n",
    "                ClusterType=redshift_cluster_type,\n",
    "                NumberOfNodes=redshift_number_of_nodes,\n",
    "                DBName=redshift_db_name,\n",
    "                MasterUsername=redshift_master_username,\n",
    "                MasterUserPassword=redshift_master_password,\n",
    "                PubliclyAccessible=True\n",
    "            )\n",
    "            wait_for_cluster_available(redshift_client, redshift_cluster_identifier)\n",
    "            prompt = f'{print_date_time()}\\t\\tCluster \"{redshift_cluster_identifier}\" is now available.'\n",
    "            print(prompt)\n",
    "            write_file('log.txt' , f\"{prompt}\")\n",
    "        else:\n",
    "            raise Exception(f'Cluster \"{redshift_cluster_identifier}\" does not exist and createRedshiftCluster=False. Aborting.')\n",
    "    add_inbound_rule(redshift_client, ec2_client, redshift_cluster_identifier)\n",
    "    associate_role_with_redshift(redshift_client, redshift_iam_role_arn, redshift_cluster_identifier)\n",
    "    turn_on_case_sensitivity(redshift_client, redshift_cluster_identifier)\n",
    "    response = redshift_client.describe_clusters(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    cluster_endpoint = response[\"Clusters\"][0][\"Endpoint\"][\"Address\"]\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=redshift_db_name,\n",
    "            user=redshift_master_username,\n",
    "            password=redshift_master_password,\n",
    "            host=cluster_endpoint,\n",
    "            port=5439\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        prompt = f'{print_date_time()}\\t\\tConnected to Redshift successfully.'\n",
    "        print(prompt)\n",
    "        write_file('log.txt' , f\"{prompt}\")\n",
    "        enable_case_sensitive_query = 'SET enable_case_sensitive_identifier TO true;'\n",
    "        cur.execute(enable_case_sensitive_query)\n",
    "        conn.commit()\n",
    "        enable_case_sensitive_query = 'SET enable_case_sensitive_identifier TO on;'\n",
    "        cur.execute(enable_case_sensitive_query)\n",
    "        conn.commit()\n",
    "        prompt = f'{print_date_time()}\\t\\tCase sensitivity enabled for this session.'\n",
    "        print(prompt)\n",
    "        write_file('log.txt' , f\"{prompt}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Failed to connect to Redshift: {e}')\n",
    "    for bucket in s3_bucket_names:\n",
    "        prompt = f'{print_date_time()}\\t\\tScanning S3 bucket: \"{bucket}\"'\n",
    "        print(prompt)\n",
    "        write_file('log.txt' , f\"{prompt}\")\n",
    "        try:\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket, Prefix=\"\")\n",
    "            if \"Contents\" not in response:\n",
    "                prompt = f'{print_date_time()}\\t\\tNo files found in bucket \"{bucket}\". Skipping...'\n",
    "                print(prompt)\n",
    "                write_file('log.txt' , f\"{prompt}\")\n",
    "                continue\n",
    "            csv_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.csv')]\n",
    "            if not csv_files:\n",
    "                prompt = f'{print_date_time()}\\t\\tNo CSV files found in bucket \"{bucket}\". Skipping...'\n",
    "                print(prompt)\n",
    "                write_file('log.txt' , f\"{prompt}\")\n",
    "                continue\n",
    "            prompt = f'{print_date_time()}\\t\\tFound {len(csv_files)} CSV file(s) in bucket \"{bucket}\". Uploading to Redshift...'\n",
    "            print(prompt)\n",
    "            write_file('log.txt' , f\"{prompt}\")\n",
    "            for csv_file in csv_files:\n",
    "                s3_path = f's3://{bucket}/{csv_file}'\n",
    "                table_name = (bucket + '-' + csv_file.split('.csv')[0])\n",
    "                check_table_query = f'''\n",
    "                SELECT * FROM information_schema.tables\n",
    "                WHERE table_schema = 'public' AND table_type = 'BASE TABLE';\n",
    "                '''\n",
    "                cur.execute(check_table_query)\n",
    "                result = cur.fetchall()\n",
    "                if table_name in [row[2] for row in result]:\n",
    "                    prompt = f'{print_date_time()}\\t\\tTable \"{table_name}\" exists. Dropping it...'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt' , f\"{prompt}\")\n",
    "                    drop_table_query = f'DROP TABLE \"{table_name}\";'\n",
    "                    cur.execute(drop_table_query)\n",
    "                    conn.commit()\n",
    "                    prompt = f'{print_date_time()}\\t\\t✅ Table \"{table_name}\" dropped.'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt' , f\"{prompt}\")\n",
    "                df = read_csv_from_s3(s3_client = s3_client, bucket_name = bucket, object_key = csv_file, dtype_str=True)\n",
    "                if df.empty:\n",
    "                    prompt = f'{print_date_time()}\\t\\tWarning: DataFrame is empty. Creating a table with default column structure.'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt' , f\"{prompt}\")\n",
    "                    create_table_query = f'CREATE TABLE \"{table_name}\" ('\n",
    "                    create_table_query += \", \".join([f'\"{col}\" TEXT' for col in df.columns])\n",
    "                    create_table_query += \");\"\n",
    "                else:\n",
    "                    global max_lengths\n",
    "                    max_lengths = df.astype(str).apply(lambda x: x.str.encode('utf-8').str.len().max()).fillna(0).astype(int)\n",
    "                    max_lengths = max_lengths.replace(0,1)\n",
    "                    cols_to_truncate = max_lengths[max_lengths > max_allowed_length].index.tolist()\n",
    "                    # for col in cols_to_truncate:\n",
    "                    #     df[col] = df[col].astype(str).apply(lambda x: truncate_with_etc(x, max_allowed_length))\n",
    "                    #     max_lengths[col] = max_allowed_length\n",
    "                    create_table_query = f'CREATE TABLE \"{table_name}\" ('\n",
    "                    create_table_query += \", \".join([f'\"{col}\" VARCHAR({length})' for col, length in max_lengths.items()])\n",
    "                    create_table_query += \");\"\n",
    "                prompt = f'{print_date_time()}\\t\\tCreating table \"{table_name}\"...'\n",
    "                print(prompt)\n",
    "                write_file('log.txt' , f\"{prompt}\")\n",
    "                cur.execute(create_table_query)\n",
    "                conn.commit()\n",
    "                prompt = f'{print_date_time()}\\t\\t✅ Table \"{table_name}\" created.'\n",
    "                print(prompt)\n",
    "                write_file('log.txt' , f\"{prompt}\")\n",
    "                copy_query = f\"\"\"\n",
    "                COPY \"{table_name}\"\n",
    "                FROM '{s3_path}'\n",
    "                IAM_ROLE '{redshift_iam_role_arn}'\n",
    "                CSV\n",
    "                IGNOREHEADER 1\n",
    "                DELIMITER ','\n",
    "                QUOTE '\"'\n",
    "                ;\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    prompt = f'{print_date_time()}\\t\\tUploading {csv_file} to Redshift table \"{table_name}\"...'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt' , f\"{prompt}\")\n",
    "                    cur.execute(copy_query)\n",
    "                    conn.commit()\n",
    "                    prompt = f'{print_date_time()}\\t\\t✅ Successfully uploaded {csv_file} to Redshift table \"{table_name}\".'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt' , f\"{prompt}\")\n",
    "                except Exception as e:\n",
    "                    prompt = f'{print_date_time()}\\t\\t❌ Error uploading {csv_file}: {e}'\n",
    "                    print(prompt)\n",
    "                    write_file('log.txt' , f\"{prompt}\")\n",
    "                    raise\n",
    "        except Exception as e:\n",
    "            prompt = f'{print_date_time()}\\t\\t❌ Error uploading files in bucket \"{bucket}\": {e}'\n",
    "            print(prompt)\n",
    "            write_file('log.txt' , f\"{prompt}\")\n",
    "            raise\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    response = redshift_client.reboot_cluster(ClusterIdentifier=redshift_cluster_identifier)\n",
    "    prompt = f'{print_date_time()}\\t\\t🚀 Upload process completed.'\n",
    "    print(prompt)\n",
    "    write_file('log.txt' , f\"{prompt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f09f95-8c7f-4d65-99f3-dcb851f30495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case : \n",
    "# from zarrinmehrlib import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
